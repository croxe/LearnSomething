Applied Regression Analysis: A Research Tool, Second Edition

John O. Rawlings
Sastry G. Pantula
David A. Dickey




CH1: Simple Linear Regression--------------------------------------------------------------

1.1 The Linear Model and Assumptions

The functional relationship between the true mean of Y, denoted by E(Y)

Y = ¦Â0 + ¦Â1*x + e  with e ~iid¡« N(0,¦Ò^2)

N(0,¦Ò^2) is normally distributed.

1.2 Least Squares Estimation

The least square estimation procedure produce the solution must give the smallest possible sum
of squared deviations of observed Y of the emstimates of their true means.

The least squares chooses b0 and b1 that minimize the sum of squares of the residuals

1.3 Predicted Values and Residuals

e residuals = Y observed - Y predicted
Which measure the discrepancy between the data and the fitted model.

residuals sum up to zeor due to least square, and there is not other choice to for b0, b1 to
provide a smaller residual square.

1.4 Analysis of Variationn in thd Dependent Variable

Y observed = Y predicted + e residuals
The squares of Y observed is called total uncorrected sum of squares

SS(Model) or RegSS is the sum of squares "accounted for" by the model;
SS(Res) or RSS is the part of sum squares that "unaccounted for".

SS(Model) = n *(Ymean )^2 + (b1)^2 *summation(Xi - Xmean)^2
          the base line +   slope * the increasement

SSTotal = (b1) ^2 *summation(Xi - Xmean)^2 + summation(residual e)^2
             RegSS                              SSR

Degree of Freedom is the sample size n - parameters. SSR contain the degrees of freedom that
is not accounted by SS(Model).

Coefficient of determination R^2 is RegSS / SSTotal Range(0,1)
The interpretation of R^2 is that 79% of the variation in the dependent variable is "explained"
by its linear relationship.

Residual mean square is an unbiased estimate of sigma square(the variance among the random errors).
Mean square expectations is regression mean square ( sigma^2 + b1(summation (Xi-Xmean)^2) )

1.5 Precision of Estimates
variance is the expectation of the squared deviation of a random variable from its mean.
It measures how far a set of random numbers are spread out from their average value.

Because Ymean, Ypredicted, e , b0, b1 ard random variables computed from the Y observed, so
measures of precision, variances or standard errors, provide a basis for judging the reliability
of the estimates.

Covariance measures the tendency of two variables to increase or decrease together.
When the random variables are independent, covariance sum up to zero.

Variance of sample mean:    Var(Ymean) = sigma^2 / n

Variance of multiple mean:

C = Ymean1 + Ymean2 -2 * Ymean3
Var(c) =  1^2 *Var(Ymean1) + 1^2 *Var(Ymean2) + (-2)^2 *Var(Ymean3)
       =  (6) (sigma^2/n)

Variance of b1:
Var(b1) = (sigma^2 / summation(Xi^2))

Variance of b0:
Var(b0) = (1/n + Xmean^2 / summation(Xi^2)) * sigma^2

Variance of Y hat at Xi:
Var(Y hat i) = [1/n + (Xi - Xmean)^2 / summation( Xj ^2)] *sigma^2

The variance attains its minimum of sigma^2/n at Xi = Xmean, when Xi is moves away from Xmean.
This formula estimate the true mean b0 + b1*Xi of Y at hte specific value Xi of X.

Variance of Predictions:
The success of the prediction will depend on how small the difference is between Y0 hat and the
future observation Y0. The difference Y0 - Y0 hat is the prediition error.
The average squared difference between E(Y0 hat - Y0)^2 is the Mean Square Error of Prediction.

Var(Y hat predicted) = [1 + 1/n + (X0 - Xmean)^2/summation(Xi^2)] *sigma^2
The variance must take account the Y itself is a random variable for predicted.

1.6 Tests of Significance and Confidence Intervals
The most common hypothesis H0 is interest about that the true value of linear regression coefficient,
the slope, is zero. That means there is no linear relationship.

Due to the nature of random variability if we test other hypothesized value; we have never get exactly
equal value even when the hypothesis is true. Therefore, the role of test of significance is to protect
against being misled by the random variation in the estimates.

Set H0 = 0 , HA != 0
If the null hypothesis is true, then b1 hat = m is normally distributed with mean zero.

t = (b1 hat - m)/ s(b1 hat)        //Note: s -- standard error

Then look at critical value of t for the apha value. 
To see whether it can provied evidence b1 is different from zero.

The F-ratio with 1 degree of freedom in the numerator is the square of the corresponding
t-statistic. Therefore, the F and the t are equivalent tests for this two-tailed
alternative hypothesis.

Confidence interval estimates

b1 +- t(0.025, degrees of freedom)*s(b1)

b0 +- t(0.025, degrees of freedom)*s(b0)

Small data set limited degrees of freedom available for estimating ¦Ò^2. 

1.7 Regression Through the Origin

In many growth models, the true mean of the dependent variable is expected to be zero when 
the value of the indpendet variable is zero.
Which means b0 is forced to be zero.

Yi = b1*Xi + ei

SS(Model)= Summation(Yi hat ^2) with n-1 degrees of freedom
SS(Res)= Summation(ei ^2) with n-1 degrees of freedom

The residual mean square is an estimate of sigma^2 when the model is correct.

Expectation of the mean in model is E[MS(Model)] = sigma^2 + b1^2 * summation(Xi^2).

The variance of b1:
Var(b1) = sigma^2/summation(Xj^2)

The divisor on sigma^2 is hte uncorrected sum of squares for independent variable, will always
be larger than the corrected sum of squares. Therefore, the estimate of b1 with no intercept
will be much more precise than b1 with intercept. Since b0 is assumed to be known.

Variance of Yi hat

Var(Yi hat) = (Xi^2/Summation(Xj^2))*sigma^2

The largest residual in the residual list is praticularly noticeable, if it is twice bigger
then the second largest residual, that indicate we maight selected wrong model to fit data.

1.8 Models with Several Independent Variables

Yi = b0 + b1*Xi1 + b2*Xi2 + ... + bn*Xin + ei

The ei are assumeh to be independent and have common variance sigma^2.
For constructing tests of significance or confidence interval statements, the random errors
are also assumed to be normally distributed.

SS(Res) = summation(Yi - Yi hat)^2

1.9 Violation of Assumptions
